{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2_Convolution_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dx8QHqb03k9"
      },
      "source": [
        "# ASSIGNMENT 2 - DEEP LEARNING FOR COMPUTER VISION\n",
        "# Convolution Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmf1U7ms2J62"
      },
      "source": [
        "The purpose of this assignment is to apply convolution networks (convnets) to image data.\n",
        "\n",
        "In this assignment, we will accomplish the following:\n",
        "1. Apply convnets to image data.\n",
        "2. Explain the relationship between sample sizes and the use of training the convnets from scratch \n",
        "versus using a pretrained network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjbhmj60jTA"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "\n",
        "Convents can be used successfully to classify images. In this assignment, you will examine the relationship between training samples and the choice of training your model from scratch, versus using a pretrained convnet. Specifically, answer the following questions:\n",
        "1. Consider the Cats & Dogs example. Start initially with a training sample of 1000, a validation sample of 500, and a test sample of 500 (like in the text). Use any technique to reduce overfitting and improve performance in developing a network that you train from scratch. What performance did you achieve?\n",
        "2. Increase your training sample size. You may pick any amount. Keep the validation and test samples the same as above. Optimize your network (again training from scratch). What performance did you achieve?\n",
        "3. Now change your training sample so that you achieve better performance than those from Steps 1 and 2. This sample size may be larger, or smaller than those in the previous steps. The objective is to find the ideal training sample size to get best prediction results.\n",
        "4. Repeat Steps 1-3, but now using a pretrained network. The sample sizes you use in Steps 2 and 3 for the pretrained network may be the same or different from those using the network where you trained from scratch. Again, use any and all optimization techniques to get best performance.\n",
        "\n",
        "Summarize your findings: what is the relationship between training sample \n",
        "size and choice of network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqzyqCacm8v_"
      },
      "source": [
        "# 1. Developing a network from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIKbKXL0__wu"
      },
      "source": [
        "We are going to develop a new neural network from scratch with a limited amount of data. Our small dataset is part of the \"Dog-vs-Cats\" dataset from Kaggle website. Our network is a convolution network, or convnet, that is a particular type of deep learning model suited for computer vision and that, thanks to its properties, provide reasonable results even when it is trained with a limited amount of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNuiOd-JABDJ"
      },
      "source": [
        "###Downloading the data\n",
        "\n",
        "Here we download an API token from a Kaggle personal account so that the algorithm can upload all the necessary data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4-4CVU4349A",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ece2e0d6-4ead-4628-9559-6f23a51f0753"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e8e76b7-6db0-40c1-ba87-3b571eaad68c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7e8e76b7-6db0-40c1-ba87-3b571eaad68c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"fabriziofiorini\",\"key\":\"8beb64c1be7e04eb78ae89a98a47349a\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n02PVMz0349A"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuhuyV90muz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ccb161-7c1c-4584-dbfb-e76a8f48d53b"
      },
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading sampleSubmission.csv to /content\n",
            "  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "100% 86.8k/86.8k [00:00<00:00, 34.5MB/s]\n",
            "Downloading train.zip to /content\n",
            " 99% 537M/543M [00:03<00:00, 164MB/s]\n",
            "100% 543M/543M [00:03<00:00, 147MB/s]\n",
            "Downloading test1.zip to /content\n",
            " 95% 258M/271M [00:01<00:00, 141MB/s]\n",
            "100% 271M/271M [00:02<00:00, 139MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk_yDdKfmz-Y"
      },
      "source": [
        "!unzip -qq train.zip"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbafMLO8m7SF"
      },
      "source": [
        "###Copying images to training, validation, and test directories\n",
        "\n",
        "Now that the dataset is loaded, we can create subsets of it for our training, validation, and test data. Specifically, we use 1000 images for the training set, 500 for the validation set, and 500 for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwCP6yyJm4D5"
      },
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small_1\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "make_subset(\"test\", start_index=1500, end_index=2000)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loddIA0hnEyQ"
      },
      "source": [
        "###Instantiating convnet for classification\n",
        "\n",
        "Let us look at the structure of our convnet. Since the input of this network is made of 3D tensor (images of a certain length, width, color), we need to first rescaling the data to a value between 0 and 1.\n",
        "\n",
        "After the first layer, we use a series of convolution with a window size of 3x3 and maxpooling with a 2x2 window. Notice that throughout the network the number of filters (depth) increases, the size of the windows remain equal, and the size of the feature maps decrease.\n",
        "\n",
        "This problem is a binary classification problem, therefore as final layer we use a dense layer that give us the probability that the output is classified as a \"cat\" or as a \"dog\". It has one output node, either \"cat\" or \"dog\". In order to feed the dense layer, we first need a layer that flatten the 3D shape in 1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk3o6xyInFWx"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCfk9ujsnJl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec03a6f-7006-48e0-aa3d-3207e71eb6cb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 180, 180, 3)]     0         \n",
            "_________________________________________________________________\n",
            "rescaling (Rescaling)        (None, 180, 180, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 178, 178, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 89, 89, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 87, 87, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 43, 43, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 41, 41, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 20, 20, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 18, 18, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 12545     \n",
            "=================================================================\n",
            "Total params: 991,041\n",
            "Trainable params: 991,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktMmXGPBUdaJ"
      },
      "source": [
        "From the model summary we can see the network structure and that we have a total of more than 991,000 parameters to optimize. Convolution networks easily tend to overfit, so we should use regularization techniques. \n",
        "\n",
        "Let us start with training the model on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfwyX1nZnMgh"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4nsUv0JYkcT"
      },
      "source": [
        "###Data preprocessing\n",
        "\n",
        "The following code will automatically convert all the images to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IArYbc-uYp5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278e5005-1d1c-4887-d70c-2c6605afeef5"
      },
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXWZvXZyb4rG"
      },
      "source": [
        "###Fitting the model using a Dataset\n",
        "\n",
        "Let us now fit the model with the training set. We use \"callbacks\" because it will automatically store a file containing the weights generating from the best epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVpPA2i5b202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f7c211-7700-471c-d4da-87793789d6fe"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch1.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 183s 3s/step - loss: 0.7266 - accuracy: 0.5125 - val_loss: 0.6924 - val_accuracy: 0.5110\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 179s 3s/step - loss: 0.7124 - accuracy: 0.5435 - val_loss: 0.6855 - val_accuracy: 0.5090\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 179s 3s/step - loss: 0.6824 - accuracy: 0.5715 - val_loss: 0.6461 - val_accuracy: 0.6290\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 176s 3s/step - loss: 0.6622 - accuracy: 0.6255 - val_loss: 0.6466 - val_accuracy: 0.6360\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.6382 - accuracy: 0.6570 - val_loss: 0.6182 - val_accuracy: 0.6550\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 174s 3s/step - loss: 0.5648 - accuracy: 0.7160 - val_loss: 0.5976 - val_accuracy: 0.6700\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 178s 3s/step - loss: 0.5448 - accuracy: 0.7410 - val_loss: 0.5813 - val_accuracy: 0.6880\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.5077 - accuracy: 0.7615 - val_loss: 0.6172 - val_accuracy: 0.7010\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 170s 3s/step - loss: 0.4457 - accuracy: 0.7900 - val_loss: 0.6402 - val_accuracy: 0.7260\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 168s 3s/step - loss: 0.3947 - accuracy: 0.8255 - val_loss: 0.5848 - val_accuracy: 0.7260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J02kQv5ccn-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "c7658afe-ab98-4aa5-e687-de1a5bbdf4c5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ppCEMekciWS"
      },
      "source": [
        "After 10 epochs, we see that the best accuracy is obtained during the fourth epoch and it is equal to 72.6%. Running the model for more epochs can lead to better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzAz0gp7crQW"
      },
      "source": [
        "###Evaluating the model on the test set\n",
        "\n",
        "Now we run the model on the test set to see what accuracy we obtain from our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKod84Wjc3rX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa61de4-b6c8-4f67-d79c-2035690a024e"
      },
      "source": [
        "test_model = keras.models.load_model(\"convnet_from_scratch1.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 21s 631ms/step - loss: 0.6116 - accuracy: 0.6770\n",
            "Test accuracy: 0.677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebVf6JFGiawf"
      },
      "source": [
        "Accuracy on the test set from the model with no data augmentation using the first partition of data is equal to 67.7%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-jRXteZeuOG"
      },
      "source": [
        "###Defining a data augmentation stage to add to an image model\n",
        "\n",
        "We are going to use data augmentation to improve the accuracy of our model. Data augmentation allow us to obtain good results even with small datasets because it generate more data from the existing training sample by applying random transformations. The model will never see the same exact image twice.\n",
        "\n",
        "In this case for example, we randomly flip, rotate, and zoom the images so that they are slightly different from the images already contained in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDWTz903eo0H"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfVNnWEEezWv"
      },
      "source": [
        "###Defining a new convnet that includes image augmentation and dropout\n",
        "\n",
        "Now we add the data augmentation step to our network at the beginning. In order to prevent overfitting, we also add dropout as the last layer before feeding the dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrAlB7I3errU"
      },
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSRSIUrDe3G-"
      },
      "source": [
        "###Training the regularized convnet\n",
        "\n",
        "Let us fit the model again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4msitMdZe6M_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde431c0-6447-48eb-9400-9c79b1cbc17b"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation1.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 179s 3s/step - loss: 0.7131 - accuracy: 0.5085 - val_loss: 0.7069 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 176s 3s/step - loss: 0.6953 - accuracy: 0.5455 - val_loss: 0.6809 - val_accuracy: 0.5080\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 176s 3s/step - loss: 0.7061 - accuracy: 0.5800 - val_loss: 0.6608 - val_accuracy: 0.5720\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 178s 3s/step - loss: 0.6785 - accuracy: 0.6285 - val_loss: 0.6800 - val_accuracy: 0.6010\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 178s 3s/step - loss: 0.6373 - accuracy: 0.6450 - val_loss: 0.5960 - val_accuracy: 0.6830\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 178s 3s/step - loss: 0.6154 - accuracy: 0.6625 - val_loss: 0.6211 - val_accuracy: 0.6810\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 178s 3s/step - loss: 0.6063 - accuracy: 0.6765 - val_loss: 0.5792 - val_accuracy: 0.6960\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 179s 3s/step - loss: 0.6065 - accuracy: 0.6900 - val_loss: 0.6417 - val_accuracy: 0.6640\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 177s 3s/step - loss: 0.5900 - accuracy: 0.6845 - val_loss: 0.5499 - val_accuracy: 0.7210\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 179s 3s/step - loss: 0.5704 - accuracy: 0.6965 - val_loss: 0.5243 - val_accuracy: 0.7220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf6a5V9gGKYq"
      },
      "source": [
        "The best validation accuracy is reached at epoch 10 with 72.2%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGXeC2E7e9pi"
      },
      "source": [
        "###Evaluating the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRLmW0Whe-3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8c9ef3-fcb8-4098-fad8-43a746a56ecb"
      },
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation1.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 20s 622ms/step - loss: 0.5745 - accuracy: 0.7080\n",
            "Test accuracy: 0.708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H14YJlFHiw-H"
      },
      "source": [
        "Accuracy on the test set from the model with data augmentation using the first partition of data is equal to 70.8%. We obtained a slightly better result. \n",
        "\n",
        "From the output we can see that data augmentation has not given an improvement on the model performance. In general, this first approach would always limited by the amount of initial data. We will see in section 4 that we can avoid this by using a pretrained network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ8lXiJ8oHLR"
      },
      "source": [
        "# 2. Increasing the training sample size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzDAm4Ogi-lY"
      },
      "source": [
        "Now, let us try to improve the model by increasing the training sample size from 1000 to 1500. Both validation set and test set sizes will not change from the first data partition. This is our second data partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EoEz0l-Lt_7"
      },
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small_2\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1500)\n",
        "make_subset(\"validation\", start_index=1500, end_index=2000)\n",
        "make_subset(\"test\", start_index=2000, end_index=2500)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AyooSDlL-It",
        "outputId": "db24df8b-fb84-4e9c-cac8-0e5244523343"
      },
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry-HQ17YxxqW"
      },
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpEul5T7oiAp",
        "outputId": "338bf502-d44c-40c8-81da-a5aae90381ea"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch2.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "94/94 [==============================] - 245s 3s/step - loss: 0.7305 - accuracy: 0.5070 - val_loss: 0.6984 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "94/94 [==============================] - 243s 3s/step - loss: 0.6838 - accuracy: 0.5857 - val_loss: 0.6352 - val_accuracy: 0.6730\n",
            "Epoch 3/10\n",
            "94/94 [==============================] - 244s 3s/step - loss: 0.6255 - accuracy: 0.6550 - val_loss: 0.7190 - val_accuracy: 0.6130\n",
            "Epoch 4/10\n",
            "94/94 [==============================] - 247s 3s/step - loss: 0.5862 - accuracy: 0.6940 - val_loss: 0.5906 - val_accuracy: 0.6810\n",
            "Epoch 5/10\n",
            "94/94 [==============================] - 245s 3s/step - loss: 0.5454 - accuracy: 0.7333 - val_loss: 0.5705 - val_accuracy: 0.6880\n",
            "Epoch 6/10\n",
            "94/94 [==============================] - 244s 3s/step - loss: 0.4943 - accuracy: 0.7610 - val_loss: 0.6393 - val_accuracy: 0.7140\n",
            "Epoch 7/10\n",
            "94/94 [==============================] - 246s 3s/step - loss: 0.4518 - accuracy: 0.7913 - val_loss: 0.5555 - val_accuracy: 0.7150\n",
            "Epoch 8/10\n",
            "94/94 [==============================] - 245s 3s/step - loss: 0.3925 - accuracy: 0.8253 - val_loss: 0.5677 - val_accuracy: 0.7390\n",
            "Epoch 9/10\n",
            "94/94 [==============================] - 245s 3s/step - loss: 0.3475 - accuracy: 0.8473 - val_loss: 0.6303 - val_accuracy: 0.7180\n",
            "Epoch 10/10\n",
            "94/94 [==============================] - 244s 3s/step - loss: 0.2782 - accuracy: 0.8813 - val_loss: 0.5981 - val_accuracy: 0.7440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYV4SzmouydY",
        "outputId": "74fd7716-87c9-4111-bdfb-f5e971b565d8"
      },
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch2.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 21s 630ms/step - loss: 0.5199 - accuracy: 0.7670\n",
            "Test accuracy: 0.767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQRNM1FwkWCq"
      },
      "source": [
        "Here validation accuracy reached 74.4%. Accuracy on the test set from the model with no data augmentation using the second partition of data is equal to 76.7%. This is the best result so far. However, by using more data for the training phase, we improved the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEFWmrlTIwC_"
      },
      "source": [
        "Let us now add a data augmentation layer to the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRwpNBkUwu1b"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ca2XMC_wy7Y"
      },
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBm-5xz3M-PS",
        "outputId": "5dc56045-0d13-4a47-b767-3315069e2cc0"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation2.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "94/94 [==============================] - 260s 3s/step - loss: 0.6944 - accuracy: 0.5117 - val_loss: 0.6901 - val_accuracy: 0.5800\n",
            "Epoch 2/10\n",
            "94/94 [==============================] - 260s 3s/step - loss: 0.6889 - accuracy: 0.5420 - val_loss: 0.6743 - val_accuracy: 0.6020\n",
            "Epoch 3/10\n",
            "94/94 [==============================] - 261s 3s/step - loss: 0.6732 - accuracy: 0.5893 - val_loss: 0.6665 - val_accuracy: 0.6050\n",
            "Epoch 4/10\n",
            "94/94 [==============================] - 265s 3s/step - loss: 0.6573 - accuracy: 0.6170 - val_loss: 0.6267 - val_accuracy: 0.6500\n",
            "Epoch 5/10\n",
            "94/94 [==============================] - 268s 3s/step - loss: 0.6341 - accuracy: 0.6463 - val_loss: 0.6361 - val_accuracy: 0.6270\n",
            "Epoch 6/10\n",
            "94/94 [==============================] - 261s 3s/step - loss: 0.6037 - accuracy: 0.6757 - val_loss: 0.5935 - val_accuracy: 0.6780\n",
            "Epoch 7/10\n",
            "94/94 [==============================] - 258s 3s/step - loss: 0.5691 - accuracy: 0.6987 - val_loss: 0.5678 - val_accuracy: 0.6950\n",
            "Epoch 8/10\n",
            "94/94 [==============================] - 259s 3s/step - loss: 0.5565 - accuracy: 0.7077 - val_loss: 0.5349 - val_accuracy: 0.7430\n",
            "Epoch 9/10\n",
            "94/94 [==============================] - 258s 3s/step - loss: 0.5429 - accuracy: 0.7290 - val_loss: 0.5464 - val_accuracy: 0.7300\n",
            "Epoch 10/10\n",
            "94/94 [==============================] - 264s 3s/step - loss: 0.5270 - accuracy: 0.7287 - val_loss: 0.5257 - val_accuracy: 0.7350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7jzT3SWM__Z",
        "outputId": "061ff411-ce2c-4844-9ac5-56e82cd9a266"
      },
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation2.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 24s 736ms/step - loss: 0.5037 - accuracy: 0.7600\n",
            "Test accuracy: 0.760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0hl1nfajUMP"
      },
      "source": [
        "Validation accuracy is now 73.5%, as we can see from the output of epoch 10. Accuracy on the test set from the model with data augmentation using the second partition of data is equal to 76.0%. Again, adding data augmentation to the model has a very small effect on improving the accuracy rate. In this case, we experienced a decrease of 0.7% compared to the model without data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Gpcz6OoNuv"
      },
      "source": [
        "# 3. Finding the ideal training sample size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmBffviTk-qM"
      },
      "source": [
        "Since we obtained better results by increasing the training sample size, now we are going to modify the amount of data in the validation set. We set the training, validation, and test set sizes, respectively, to 1500, 1000, and 500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK297n94if87"
      },
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small_3\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1500)\n",
        "make_subset(\"validation\", start_index=1500, end_index=2500)\n",
        "make_subset(\"test\", start_index=2500, end_index=3000)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQSsBnA2ilC4",
        "outputId": "86d21392-b8e5-4be0-93aa-6b16f214f53c"
      },
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 files belonging to 2 classes.\n",
            "Found 2000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnk_FPr_y_QI"
      },
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7SPWDmPjVwr",
        "outputId": "2e4cca1f-bbcd-441b-da57-2fbebed893b4"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch3.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "94/94 [==============================] - 352s 4s/step - loss: 0.7234 - accuracy: 0.5413 - val_loss: 0.6873 - val_accuracy: 0.5205\n",
            "Epoch 2/10\n",
            "94/94 [==============================] - 348s 4s/step - loss: 0.6785 - accuracy: 0.6123 - val_loss: 0.6674 - val_accuracy: 0.6135\n",
            "Epoch 3/10\n",
            "94/94 [==============================] - 285s 3s/step - loss: 0.6163 - accuracy: 0.6633 - val_loss: 0.6216 - val_accuracy: 0.6805\n",
            "Epoch 4/10\n",
            "94/94 [==============================] - 286s 3s/step - loss: 0.5813 - accuracy: 0.7023 - val_loss: 0.5740 - val_accuracy: 0.6945\n",
            "Epoch 5/10\n",
            "94/94 [==============================] - 277s 3s/step - loss: 0.5222 - accuracy: 0.7397 - val_loss: 0.5938 - val_accuracy: 0.7170\n",
            "Epoch 6/10\n",
            "94/94 [==============================] - 272s 3s/step - loss: 0.4735 - accuracy: 0.7727 - val_loss: 0.6736 - val_accuracy: 0.6870\n",
            "Epoch 7/10\n",
            "94/94 [==============================] - 273s 3s/step - loss: 0.4229 - accuracy: 0.8027 - val_loss: 0.6469 - val_accuracy: 0.6965\n",
            "Epoch 8/10\n",
            "94/94 [==============================] - 277s 3s/step - loss: 0.3678 - accuracy: 0.8397 - val_loss: 0.5737 - val_accuracy: 0.7515\n",
            "Epoch 9/10\n",
            "94/94 [==============================] - 276s 3s/step - loss: 0.3022 - accuracy: 0.8733 - val_loss: 0.6441 - val_accuracy: 0.7565\n",
            "Epoch 10/10\n",
            "94/94 [==============================] - 271s 3s/step - loss: 0.2459 - accuracy: 0.9023 - val_loss: 0.7250 - val_accuracy: 0.7370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duujxuyJjXky",
        "outputId": "b1e14ca6-c9ee-4663-881a-73d97676fac4"
      },
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch3.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 21s 641ms/step - loss: 0.6289 - accuracy: 0.7350\n",
            "Test accuracy: 0.735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLCWCIwzlbsr"
      },
      "source": [
        "With the third partition, we otained a validation accuracy of 75.65% while the accuracy on the test set is equal to 73.5%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpIi2FLgIpLX"
      },
      "source": [
        "Let us now add a data augmentation layer to the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgK4gZeZzhZS"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hOUEmHCzbwu"
      },
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDD54XM87_dI",
        "outputId": "7d92fe91-cfd8-4c3e-a750-77289887a0a1"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation3.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "94/94 [==============================] - 283s 3s/step - loss: 0.6934 - accuracy: 0.5320 - val_loss: 0.6788 - val_accuracy: 0.5845\n",
            "Epoch 2/10\n",
            "94/94 [==============================] - 278s 3s/step - loss: 0.6683 - accuracy: 0.6037 - val_loss: 0.6598 - val_accuracy: 0.6050\n",
            "Epoch 3/10\n",
            "94/94 [==============================] - 275s 3s/step - loss: 0.6545 - accuracy: 0.6213 - val_loss: 0.6550 - val_accuracy: 0.6175\n",
            "Epoch 4/10\n",
            "94/94 [==============================] - 275s 3s/step - loss: 0.6308 - accuracy: 0.6513 - val_loss: 0.6384 - val_accuracy: 0.6445\n",
            "Epoch 5/10\n",
            "94/94 [==============================] - 277s 3s/step - loss: 0.6152 - accuracy: 0.6663 - val_loss: 0.6008 - val_accuracy: 0.6845\n",
            "Epoch 6/10\n",
            "94/94 [==============================] - 276s 3s/step - loss: 0.5942 - accuracy: 0.6873 - val_loss: 0.5874 - val_accuracy: 0.6920\n",
            "Epoch 7/10\n",
            "94/94 [==============================] - 283s 3s/step - loss: 0.5687 - accuracy: 0.7080 - val_loss: 0.5720 - val_accuracy: 0.7200\n",
            "Epoch 8/10\n",
            "94/94 [==============================] - 283s 3s/step - loss: 0.5448 - accuracy: 0.7263 - val_loss: 0.5874 - val_accuracy: 0.7180\n",
            "Epoch 9/10\n",
            "94/94 [==============================] - 285s 3s/step - loss: 0.5436 - accuracy: 0.7303 - val_loss: 0.5528 - val_accuracy: 0.7260\n",
            "Epoch 10/10\n",
            "94/94 [==============================] - 286s 3s/step - loss: 0.5281 - accuracy: 0.7313 - val_loss: 0.5380 - val_accuracy: 0.7345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLYWdm7X7_7Y",
        "outputId": "1ca59191-207c-4b43-865a-e7d3d8dbe9a7"
      },
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation3.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 21s 657ms/step - loss: 0.5575 - accuracy: 0.7240\n",
            "Test accuracy: 0.724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHMHWRY6lodx"
      },
      "source": [
        "Accuracy on the validation set is 73.45%. Accuracy on the test set from the model with data augmentation using the third partition of data is equal to 72.4%. \n",
        "\n",
        "We did not improve our model's performance by increasing the validation sample size. Hence, considering our different versions of the model we developed from scratch, we reached the best accuracy with the two models trained with the second partition: 1500 on training set, 500 on validation set, and 500 on test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGLo3xPLoY7_"
      },
      "source": [
        "# 4. Using a pretrained network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY4e99Yig5ud"
      },
      "source": [
        "For the last section of the assignment we want to use a pretrained network, that is a network that was previously trained on a large dataset. If this original dataset is large enough and general enough, then the pretrained network can effectively act as a generic model and its features can prove useful for many different computer vision problems. Such portability of learned features across different problems is a key advantage of deep learning compared to other machine learning approaches.\n",
        "\n",
        "Let us consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and dogs. The architecture of the network is called VGG16. Its a simple and widely used convnet architecture for ImageNet.\n",
        "\n",
        "There are two ways to use a pretrained network: \n",
        "- feature extraction\n",
        "- fine-tuning\n",
        "\n",
        "In this assignment, we are going to apply feature extraction, first without data augmentation and then including it to reach even better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy1iMqk3jucO"
      },
      "source": [
        "###Feature extraction - Instantiating the VGG16 convolutional base\n",
        "\n",
        "Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch. In other words, we are going to use the convolutional base, that is the series of pooling and convolution layers we had in our network, and then pass the output on a densely connected classifier that we choose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPGQSclNj08H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf5a813-47b0-4fe6-a776-4d654a1a1ec3"
      },
      "source": [
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(180, 180, 3))\n",
        "conv_base.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 180, 180, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 180, 180, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 180, 180, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 90, 90, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 90, 90, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 90, 90, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 45, 45, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 45, 45, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 45, 45, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 45, 45, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 22, 22, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 22, 22, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 22, 22, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 22, 22, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 11, 11, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCqmmX7Ok5Al"
      },
      "source": [
        "This convolutional base has 14.7 million parameters. Notice that we started with the same input we used previously, 3D tensors of shape (180, 180, 3), and we ended up with a tensor of shape (5, 5, 512)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pybcvUfCms_n"
      },
      "source": [
        "###Feature extraction - Extracting features and corresponding labels\n",
        "\n",
        "Here we preprocess the images before they are sent to the pretrained network. We can check the shape of the output and see that it corrisponds to what we saw in the network structure: (5, 5, 512)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LsTUSZbm0g9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "7ad0e7f9-b897-4453-acdb-dc9d10405078"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_features_and_labels(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for images, labels in dataset:\n",
        "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "        features = conv_base.predict(preprocessed_images)\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
        "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
        "test_features, test_labels =  get_features_and_labels(test_dataset)\n",
        "\n",
        "train_features.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f16c3ad1b26b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_features_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_features_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_features_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f16c3ad1b26b>\u001b[0m in \u001b[0;36mget_features_and_labels\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpreprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lp_dxTNnDdh"
      },
      "source": [
        "###Feature extraction - Defining and training the densely connected classifier\n",
        "\n",
        "Now that we used the convolutional base of the pretrained network, we can feed the dense layer and train it. Notice that shape of the input in the following code is the same of the output of the convolutional base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEM892U5m3nQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba8f1b4-5b28-45d3-cbaa-e9884f72b8c3"
      },
      "source": [
        "inputs = keras.Input(shape=(5, 5, 512))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"feature_extractionPT1.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=15,\n",
        "    validation_data=(val_features, val_labels),\n",
        "    callbacks=callbacks)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "94/94 [==============================] - 5s 52ms/step - loss: 15.2194 - accuracy: 0.9330 - val_loss: 3.3935 - val_accuracy: 0.9725\n",
            "Epoch 2/15\n",
            "94/94 [==============================] - 5s 51ms/step - loss: 3.0662 - accuracy: 0.9790 - val_loss: 4.0633 - val_accuracy: 0.9745\n",
            "Epoch 3/15\n",
            "94/94 [==============================] - 5s 52ms/step - loss: 1.4064 - accuracy: 0.9873 - val_loss: 3.9202 - val_accuracy: 0.9715\n",
            "Epoch 4/15\n",
            "94/94 [==============================] - 5s 53ms/step - loss: 1.4706 - accuracy: 0.9910 - val_loss: 10.0884 - val_accuracy: 0.9590\n",
            "Epoch 5/15\n",
            "94/94 [==============================] - 5s 50ms/step - loss: 1.1365 - accuracy: 0.9920 - val_loss: 4.6114 - val_accuracy: 0.9780\n",
            "Epoch 6/15\n",
            "94/94 [==============================] - 5s 50ms/step - loss: 0.5924 - accuracy: 0.9943 - val_loss: 5.0588 - val_accuracy: 0.9770\n",
            "Epoch 7/15\n",
            "94/94 [==============================] - 5s 51ms/step - loss: 0.3519 - accuracy: 0.9973 - val_loss: 5.8654 - val_accuracy: 0.9750\n",
            "Epoch 8/15\n",
            "94/94 [==============================] - 5s 51ms/step - loss: 0.9847 - accuracy: 0.9933 - val_loss: 6.1058 - val_accuracy: 0.9755\n",
            "Epoch 9/15\n",
            "94/94 [==============================] - 5s 52ms/step - loss: 0.4488 - accuracy: 0.9957 - val_loss: 7.4364 - val_accuracy: 0.9730\n",
            "Epoch 10/15\n",
            "94/94 [==============================] - 5s 52ms/step - loss: 0.3461 - accuracy: 0.9960 - val_loss: 5.5139 - val_accuracy: 0.9765\n",
            "Epoch 11/15\n",
            "94/94 [==============================] - 5s 52ms/step - loss: 0.0385 - accuracy: 0.9993 - val_loss: 5.7862 - val_accuracy: 0.9775\n",
            "Epoch 12/15\n",
            "94/94 [==============================] - 5s 51ms/step - loss: 0.2262 - accuracy: 0.9980 - val_loss: 7.3332 - val_accuracy: 0.9695\n",
            "Epoch 13/15\n",
            "94/94 [==============================] - 5s 53ms/step - loss: 0.1328 - accuracy: 0.9980 - val_loss: 8.4311 - val_accuracy: 0.9700\n",
            "Epoch 14/15\n",
            "94/94 [==============================] - 5s 53ms/step - loss: 0.0795 - accuracy: 0.9983 - val_loss: 5.4274 - val_accuracy: 0.9790\n",
            "Epoch 15/15\n",
            "94/94 [==============================] - 5s 53ms/step - loss: 0.1029 - accuracy: 0.9983 - val_loss: 6.8529 - val_accuracy: 0.9765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD9g6wvNnMuQ"
      },
      "source": [
        "###Feature extraction - Plotting the results\n",
        "\n",
        "From the results we see that the accuracy improved notably from the performance we obtained when we trained a model from scratch using our limited dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7jJmO7lnSSu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "e6c777c4-bc3d-4a65-e0a4-3f5ae7576d53"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "byDGq50zCMlf",
        "outputId": "43dae1de-37b7-4b94-9013-f15c4d94ee27"
      },
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"feature_extractionPT1.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3bcc4749874e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_model = keras.models.load_model(\n\u001b[0;32m----> 2\u001b[0;31m     \"feature_extraction.keras\")\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test accuracy: {test_acc:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[0;32m--> 121\u001b[0;31m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: feature_extraction.keras/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8LEKEiCh7Nq"
      },
      "source": [
        "###Feature extraction with Data Augmentation\n",
        "\n",
        "Let us now add a data augmentation layer to the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWciqvLFCyE2"
      },
      "source": [
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)\n",
        "conv_base.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8AOg-HgC3n6"
      },
      "source": [
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6QxSz46C4NL"
      },
      "source": [
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61hfWQjbC5qv"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = keras.applications.vgg16.preprocess_input(x)\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F-Z1k_VDEn_"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentationPT2.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=5,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scdzNENaDEie"
      },
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentationPT2.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG6PU7IRue84"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2py_xCqups0"
      },
      "source": [
        "We saw that the performance of the models we created depends on different factors. The biggest impact is given by the initial choice of either training a model from scratch or using a pretrained model. Even with the best configuration of the model we created, it is unlikely that it can achieve the accuracy we saw with the pretrained network.\n",
        "\n",
        "However, we were able to reach a decent performance even if we trained the model with a very small amount of data. From the different configurations, it seems that the biggest impact on the performance is given by a larger training set (second and third partitions) but not by a larger validation set (third partition). We could try to develop our model using other partitions and check if it is possible to get a better result. Another way to further improve our network is to increase the number of epochs, since we saw from the plot that the validation accuracy was still improving, even if marginally. Finally, it is possible that modifying the network structure (number of layers, type of loss metric, activation functions) it leads to a better performance.\n",
        "\n",
        "The data augmentation technique helped us on the first data partition (+3.1%), when the training set size was particularly small. When we increased its size, it became less relevant on improving the accuracy of the model. "
      ]
    }
  ]
}