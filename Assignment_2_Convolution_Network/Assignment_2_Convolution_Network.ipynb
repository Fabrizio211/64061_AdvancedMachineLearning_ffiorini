{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2_Convolution_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dx8QHqb03k9"
      },
      "source": [
        "# ASSIGNMENT 2 - DEEP LEARNING FOR COMPUTER VISION\n",
        "# Convolution Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmf1U7ms2J62"
      },
      "source": [
        "The purpose of this assignment is to apply convolution networks (convnets) to image data.\n",
        "\n",
        "In this assignment, we will accomplish the following:\n",
        "1. Apply convnets to image data.\n",
        "2. Explain the relationship between sample sizes and the use of training the convnets from scratch \n",
        "versus using a pretrained network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjbhmj60jTA"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "\n",
        "Convents can be used successfully to classify images. In this assignment, you will examine the relationship between training samples and the choice of training your model from scratch, versus using a pretrained convnet. Specifically, answer the following questions:\n",
        "1. Consider the Cats & Dogs example. Start initially with a training sample of 1000, a validation sample of 500, and a test sample of 500 (like in the text). Use any technique to reduce overfitting and improve performance in developing a network that you train from scratch. What performance did you achieve?\n",
        "2. Increase your training sample size. You may pick any amount. Keep the validation and test samples the same as above. Optimize your network (again training from scratch). What performance did you achieve?\n",
        "3. Now change your training sample so that you achieve better performance than those from Steps 1 and 2. This sample size may be larger, or smaller than those in the previous steps. The objective is to find the ideal training sample size to get best prediction results.\n",
        "4. Repeat Steps 1-3, but now using a pretrained network. The sample sizes you use in Steps 2 and 3 for the pretrained network may be the same or different from those using the network where you trained from scratch. Again, use any and all optimization techniques to get best performance.\n",
        "\n",
        "Summarize your findings: what is the relationship between training sample \n",
        "size and choice of network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqzyqCacm8v_"
      },
      "source": [
        "# 1. Developing a network from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIKbKXL0__wu"
      },
      "source": [
        "We are going to develop a new neural network from scratch with a limited amount of data. Our small dataset is part of the \"Dog-vs-Cats\" dataset from Kaggle website. Our network is a convolution network, or convnet, that is a particular type of deep learning model suited for computer vision and that, thanks to its properties, provide reasonable results even when it is trained with a limited amount of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNuiOd-JABDJ"
      },
      "source": [
        "###Downloading the data\n",
        "\n",
        "Here we download an API token from a Kaggle personal account so that the algorithm can upload all the necessary data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4-4CVU4349A",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "56420f4b-69f9-4723-d6c7-022f4fadc2c4"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a97093b2-cc36-4a7d-bc70-bed063205042\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a97093b2-cc36-4a7d-bc70-bed063205042\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"fabriziofiorini\",\"key\":\"8beb64c1be7e04eb78ae89a98a47349a\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n02PVMz0349A"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuhuyV90muz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fbcd15-c0fd-4818-dbd6-c5be91bfb0bd"
      },
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.zip to /content\n",
            " 98% 533M/543M [00:03<00:00, 170MB/s]\n",
            "100% 543M/543M [00:03<00:00, 174MB/s]\n",
            "Downloading test1.zip to /content\n",
            " 97% 262M/271M [00:02<00:00, 132MB/s]\n",
            "100% 271M/271M [00:02<00:00, 125MB/s]\n",
            "Downloading sampleSubmission.csv to /content\n",
            "  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "100% 86.8k/86.8k [00:00<00:00, 88.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk_yDdKfmz-Y"
      },
      "source": [
        "!unzip -qq train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbafMLO8m7SF"
      },
      "source": [
        "###Copying images to training, validation, and test directories\n",
        "\n",
        "Now that the dataset is loaded, we can create subsets of it for our training, validation, and test data. Specifically, we use 1000 images for the training set, 500 for the validation set, and 500 for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwCP6yyJm4D5"
      },
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train1\", start_index=0, end_index=1000)\n",
        "make_subset(\"validation1\", start_index=1000, end_index=1500)\n",
        "make_subset(\"test1\", start_index=1500, end_index=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loddIA0hnEyQ"
      },
      "source": [
        "###Instantiating convnet for classification\n",
        "\n",
        "Let us look at the structure of our convnet. Since the input of this network is made of 3D tensor (images of a certain length, width, color), we need to first rescaling the data to a value between 0 and 1.\n",
        "\n",
        "After the first layer, we use a series of convolution with a window size of 3x3 and maxpooling with a 2x2 window. Notice that throughout the network the number of filters (depth) increases, the size of the windows remain equal, and the size of the feature maps decrease.\n",
        "\n",
        "This problem is a binary classification problem, therefore as final layer we use a dense layer that give us the probability that the output is classified as a \"cat\" or as a \"dog\". It has one output node, either \"cat\" or \"dog\". In order to feed the dense layer, we first need a layer that flatten the 3D shape in 1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk3o6xyInFWx"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs1 = keras.Input(shape=(180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs1)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs1 = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model1 = keras.Model(inputs=inputs1, outputs=outputs1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCfk9ujsnJl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07fc101d-bac2-42ee-d2ec-c34091ae4a97"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 180, 180, 3)]     0         \n",
            "_________________________________________________________________\n",
            "rescaling (Rescaling)        (None, 180, 180, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 178, 178, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 89, 89, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 87, 87, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 43, 43, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 41, 41, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 20, 20, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 18, 18, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 7, 7, 256)         590080    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 12545     \n",
            "=================================================================\n",
            "Total params: 991,041\n",
            "Trainable params: 991,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktMmXGPBUdaJ"
      },
      "source": [
        "From the model summary we can see the network structure and that we have a total of more than 991,000 parameters to optimize. Convolution networks easily tend to overfit, so we should use regularization techniques. \n",
        "\n",
        "Let us start with training the model on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfwyX1nZnMgh"
      },
      "source": [
        "model1.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4nsUv0JYkcT"
      },
      "source": [
        "###Data preprocessing\n",
        "\n",
        "The following code will automatically convert all the images to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IArYbc-uYp5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1168dc9e-aff2-420e-9bc6-93322016ccf9"
      },
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset1 = image_dataset_from_directory(\n",
        "    new_base_dir / \"train1\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset1 = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation1\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset1 = image_dataset_from_directory(\n",
        "    new_base_dir / \"test1\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXWZvXZyb4rG"
      },
      "source": [
        "###Fitting the model using a Dataset\n",
        "\n",
        "Let us now fit the model with the training set. We use \"callbacks\" because it will automatically store a file containing the weights generating from the best epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVpPA2i5b202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdab79ee-b35d-42d6-c95c-69eb391d272d"
      },
      "source": [
        "callbacks1 = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history1 = model1.fit(\n",
        "    train_dataset1,\n",
        "    epochs=5,\n",
        "    validation_data=validation_dataset1,\n",
        "    callbacks=callbacks1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "63/63 [==============================] - 161s 3s/step - loss: 0.7528 - accuracy: 0.5170 - val_loss: 0.6900 - val_accuracy: 0.5340\n",
            "Epoch 2/5\n",
            "63/63 [==============================] - 160s 3s/step - loss: 0.7187 - accuracy: 0.5445 - val_loss: 0.6699 - val_accuracy: 0.5630\n",
            "Epoch 3/5\n",
            "63/63 [==============================] - 159s 3s/step - loss: 0.6724 - accuracy: 0.5975 - val_loss: 0.6891 - val_accuracy: 0.5440\n",
            "Epoch 4/5\n",
            "63/63 [==============================] - 160s 3s/step - loss: 0.6722 - accuracy: 0.6395 - val_loss: 0.6272 - val_accuracy: 0.6460\n",
            "Epoch 5/5\n",
            "63/63 [==============================] - 159s 3s/step - loss: 0.6214 - accuracy: 0.6820 - val_loss: 0.6325 - val_accuracy: 0.6420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J02kQv5ccn-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "99098c2d-95a2-48ca-f409-c2bf2086b0d4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "accuracy = history1.history[\"accuracy\"]\n",
        "val_accuracy = history1.history[\"val_accuracy\"]\n",
        "loss = history1.history[\"loss\"]\n",
        "val_loss = history1.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ppCEMekciWS"
      },
      "source": [
        "After 5 epochs, we see that the best accuracy is obtained during the fourth epoch and it is equal to 0.646. Running the model for more epochs can lead to better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzAz0gp7crQW"
      },
      "source": [
        "###Evaluating the model on the test set\n",
        "\n",
        "Now we run the model on the test set to see what accuracy we obtain from our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKod84Wjc3rX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd1b83f-6a73-4158-b087-50b9daf2cf12"
      },
      "source": [
        "test_model1 = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model1.evaluate(test_dataset1)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 20s 619ms/step - loss: 0.6562 - accuracy: 0.6160\n",
            "Test accuracy: 0.616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebVf6JFGiawf"
      },
      "source": [
        "Accuracy on the test set from the model 1 (no data augmentation) using the first partition of data is equal to 61.6%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-jRXteZeuOG"
      },
      "source": [
        "###Defining a data augmentation stage to add to an image model\n",
        "\n",
        "We are going to use data augmentation to improve the accuracy of our model. Data augmentation allow us to obtain good results even with small datasets because it generate more data from the existing training sample by applying random transformations. The model will never see the same exact image twice.\n",
        "\n",
        "In this case for example, we randomly flip, rotate, and zoom the images so that they are slightly different from the images already contained in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDWTz903eo0H"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfVNnWEEezWv"
      },
      "source": [
        "###Defining a new convnet that includes image augmentation and dropout\n",
        "\n",
        "Now we add the data augmentation step to our network at the beginning. In order to prevent overfitting, we also add dropout as the last layer before feeding the dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrAlB7I3errU"
      },
      "source": [
        "inputs2 = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs2)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs2 = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model2 = keras.Model(inputs=inputs2, outputs=outputs2)\n",
        "\n",
        "model2.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSRSIUrDe3G-"
      },
      "source": [
        "###Training the regularized convnet\n",
        "\n",
        "Let us fit the model again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4msitMdZe6M_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e516d1b1-6c9e-4d92-e253-6ef725d886ee"
      },
      "source": [
        "callbacks2 = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history2 = model2.fit(\n",
        "    train_dataset1,\n",
        "    epochs=5,\n",
        "    validation_data=validation_dataset1,\n",
        "    callbacks=callbacks2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "63/63 [==============================] - 171s 3s/step - loss: 0.8050 - accuracy: 0.5090 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "63/63 [==============================] - 169s 3s/step - loss: 0.6952 - accuracy: 0.5210 - val_loss: 0.6929 - val_accuracy: 0.5020\n",
            "Epoch 3/5\n",
            "63/63 [==============================] - 169s 3s/step - loss: 0.6930 - accuracy: 0.5255 - val_loss: 0.6841 - val_accuracy: 0.5040\n",
            "Epoch 4/5\n",
            "63/63 [==============================] - 169s 3s/step - loss: 0.7069 - accuracy: 0.5850 - val_loss: 0.7487 - val_accuracy: 0.5830\n",
            "Epoch 5/5\n",
            "63/63 [==============================] - 168s 3s/step - loss: 0.6583 - accuracy: 0.6290 - val_loss: 0.6131 - val_accuracy: 0.6450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGXeC2E7e9pi"
      },
      "source": [
        "###Evaluating the model on the test set\n",
        "\n",
        "From the output we can see that data augmentation has not given an improvement on the model performance. In general, this first approach would always limited by the amount of initial data. We will see in section 4 that we can avoid this by using a pretrained network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRLmW0Whe-3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1f43dc-61a9-4a14-dac3-f44e043e704e"
      },
      "source": [
        "test_model2 = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model2.evaluate(test_dataset1)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 21s 627ms/step - loss: 0.6382 - accuracy: 0.6470\n",
            "Test accuracy: 0.647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H14YJlFHiw-H"
      },
      "source": [
        "Accuracy on the test set from the model 2 (with data augmentation) using the first partition of data is equal to 64.7%. We obtained a slightly better result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ8lXiJ8oHLR"
      },
      "source": [
        "# 2. Increasing the training sample size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzDAm4Ogi-lY"
      },
      "source": [
        "Now, let us try to improve the model by increasing the training sample size from 1000 to 1300. The validation set size will be equal to 300 and the test set size to 400. This is our second data partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EoEz0l-Lt_7"
      },
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train2\", start_index=0, end_index=1300)\n",
        "make_subset(\"validation2\", start_index=1300, end_index=1600)\n",
        "make_subset(\"test2\", start_index=1600, end_index=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AyooSDlL-It",
        "outputId": "3ae1faf6-e15d-4581-e5a4-be391823a281"
      },
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset2 = image_dataset_from_directory(\n",
        "    new_base_dir / \"train2\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset2 = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation2\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset2 = image_dataset_from_directory(\n",
        "    new_base_dir / \"test2\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2600 files belonging to 2 classes.\n",
            "Found 600 files belonging to 2 classes.\n",
            "Found 800 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpEul5T7oiAp",
        "outputId": "e795d778-cc5c-4246-a665-77ad2905db39"
      },
      "source": [
        "callbacks3 = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history3 = model1.fit(\n",
        "    train_dataset2,\n",
        "    epochs=5,\n",
        "    validation_data=validation_dataset2,\n",
        "    callbacks=callbacks3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "82/82 [==============================] - 195s 2s/step - loss: 0.5945 - accuracy: 0.6823 - val_loss: 0.6021 - val_accuracy: 0.6633\n",
            "Epoch 2/5\n",
            "82/82 [==============================] - 195s 2s/step - loss: 0.5438 - accuracy: 0.7350 - val_loss: 0.6313 - val_accuracy: 0.6383\n",
            "Epoch 3/5\n",
            "82/82 [==============================] - 195s 2s/step - loss: 0.4748 - accuracy: 0.7835 - val_loss: 0.6327 - val_accuracy: 0.6983\n",
            "Epoch 4/5\n",
            "82/82 [==============================] - 195s 2s/step - loss: 0.4097 - accuracy: 0.8192 - val_loss: 0.5705 - val_accuracy: 0.7550\n",
            "Epoch 5/5\n",
            "82/82 [==============================] - 195s 2s/step - loss: 0.3545 - accuracy: 0.8465 - val_loss: 0.6046 - val_accuracy: 0.7483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYV4SzmouydY",
        "outputId": "10c0100b-ca57-44a8-a308-6d69adef2b05"
      },
      "source": [
        "test_model3 = keras.models.load_model(\n",
        "    \"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model3.evaluate(test_dataset2)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 16s 629ms/step - loss: 0.6476 - accuracy: 0.7025\n",
            "Test accuracy: 0.702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQRNM1FwkWCq"
      },
      "source": [
        "Accuracy on the test set from the model 1 (no data augmentation) using the second partition of data is equal to 70.2%. By using more data for the training phase, we improved the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBm-5xz3M-PS",
        "outputId": "9ae191c6-cb80-43bc-eec6-01fbb7f1d641"
      },
      "source": [
        "callbacks4 = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history4 = model2.fit(\n",
        "    train_dataset2,\n",
        "    epochs=5,\n",
        "    validation_data=validation_dataset2,\n",
        "    callbacks=callbacks4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "82/82 [==============================] - 206s 3s/step - loss: 0.6406 - accuracy: 0.6331 - val_loss: 0.6713 - val_accuracy: 0.6000\n",
            "Epoch 2/5\n",
            "82/82 [==============================] - 206s 3s/step - loss: 0.6254 - accuracy: 0.6665 - val_loss: 0.8245 - val_accuracy: 0.5467\n",
            "Epoch 3/5\n",
            "82/82 [==============================] - 206s 3s/step - loss: 0.6016 - accuracy: 0.6823 - val_loss: 0.6913 - val_accuracy: 0.5967\n",
            "Epoch 4/5\n",
            "82/82 [==============================] - 206s 3s/step - loss: 0.5969 - accuracy: 0.6796 - val_loss: 0.5773 - val_accuracy: 0.6967\n",
            "Epoch 5/5\n",
            "82/82 [==============================] - 207s 3s/step - loss: 0.5798 - accuracy: 0.6892 - val_loss: 1.3537 - val_accuracy: 0.5783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7jzT3SWM__Z",
        "outputId": "ce3a8892-ed44-43d7-f4a0-701a920ded93"
      },
      "source": [
        "test_model4 = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model4.evaluate(test_dataset2)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 17s 641ms/step - loss: 0.5721 - accuracy: 0.7100\n",
            "Test accuracy: 0.710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0hl1nfajUMP"
      },
      "source": [
        "Accuracy on the test set from the model 2 (with data augmentation) using the second partition of data is equal to 71.0%. Again, adding data augmentation to the model has a very small effect on improving the accuracy rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Gpcz6OoNuv"
      },
      "source": [
        "# 3. Finding the ideal training sample size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmBffviTk-qM"
      },
      "source": [
        "Now we want to try to reduce the amount of data on the training sample. We set the training, validation, and test set sizes, respectively, to 800, 600, and 600."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK297n94if87"
      },
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train3\", start_index=0, end_index=800)\n",
        "make_subset(\"validation3\", start_index=800, end_index=1400)\n",
        "make_subset(\"test3\", start_index=1400, end_index=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQSsBnA2ilC4",
        "outputId": "b7d87204-9901-4c87-b146-04846c06dcfe"
      },
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset3 = image_dataset_from_directory(\n",
        "    new_base_dir / \"train3\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset3 = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation3\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset3 = image_dataset_from_directory(\n",
        "    new_base_dir / \"test3\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1600 files belonging to 2 classes.\n",
            "Found 1200 files belonging to 2 classes.\n",
            "Found 1200 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7SPWDmPjVwr",
        "outputId": "a54abfff-e287-4365-ff2c-b6eab04318de"
      },
      "source": [
        "callbacks5 = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history5 = model1.fit(\n",
        "    train_dataset3,\n",
        "    epochs=5,\n",
        "    validation_data=validation_dataset3,\n",
        "    callbacks=callbacks5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "50/50 [==============================] - 137s 3s/step - loss: 0.2702 - accuracy: 0.8969 - val_loss: 0.3356 - val_accuracy: 0.8575\n",
            "Epoch 2/5\n",
            "50/50 [==============================] - 137s 3s/step - loss: 0.2294 - accuracy: 0.9075 - val_loss: 0.3372 - val_accuracy: 0.8742\n",
            "Epoch 3/5\n",
            "50/50 [==============================] - 137s 3s/step - loss: 0.1689 - accuracy: 0.9394 - val_loss: 0.3652 - val_accuracy: 0.8600\n",
            "Epoch 4/5\n",
            "50/50 [==============================] - 137s 3s/step - loss: 0.1267 - accuracy: 0.9450 - val_loss: 0.3994 - val_accuracy: 0.8625\n",
            "Epoch 5/5\n",
            "50/50 [==============================] - 137s 3s/step - loss: 0.1071 - accuracy: 0.9613 - val_loss: 0.7077 - val_accuracy: 0.8125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duujxuyJjXky",
        "outputId": "f7341b5e-aee3-488e-aff1-2e792c0c10dc"
      },
      "source": [
        "test_model5 = keras.models.load_model(\n",
        "    \"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model5.evaluate(test_dataset3)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 25s 639ms/step - loss: 0.6201 - accuracy: 0.7183\n",
            "Test accuracy: 0.718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLCWCIwzlbsr"
      },
      "source": [
        "Accuracy on the test set from the model 1 (no data augmentation) using the third partition of data is equal to 71.8%. It is the best result so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDD54XM87_dI",
        "outputId": "005b0c90-45f4-4e46-d219-1c57b21e770f"
      },
      "source": [
        "callbacks6 = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history6 = model2.fit(\n",
        "    train_dataset3,\n",
        "    epochs=5,\n",
        "    validation_data=validation_dataset3,\n",
        "    callbacks=callbacks6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "50/50 [==============================] - 144s 3s/step - loss: 0.5763 - accuracy: 0.7094 - val_loss: 0.5238 - val_accuracy: 0.7483\n",
            "Epoch 2/5\n",
            "50/50 [==============================] - 144s 3s/step - loss: 0.5360 - accuracy: 0.7350 - val_loss: 0.5343 - val_accuracy: 0.7333\n",
            "Epoch 3/5\n",
            "50/50 [==============================] - 144s 3s/step - loss: 0.5262 - accuracy: 0.7462 - val_loss: 0.4945 - val_accuracy: 0.7658\n",
            "Epoch 4/5\n",
            "50/50 [==============================] - 145s 3s/step - loss: 0.5177 - accuracy: 0.7469 - val_loss: 0.4602 - val_accuracy: 0.7767\n",
            "Epoch 5/5\n",
            "50/50 [==============================] - 145s 3s/step - loss: 0.5245 - accuracy: 0.7369 - val_loss: 0.4640 - val_accuracy: 0.7900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLYWdm7X7_7Y",
        "outputId": "f7315c45-a8f2-4d5f-cfaa-bec99aca20f6"
      },
      "source": [
        "test_model6 = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model6.evaluate(test_dataset3)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 25s 641ms/step - loss: 0.5219 - accuracy: 0.7442\n",
            "Test accuracy: 0.744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHMHWRY6lodx"
      },
      "source": [
        "Accuracy on the test set from the model 2 (with data augmentation) using the third partition of data is equal to 74.4%. This result is another improvement on our search for the best allocation of data among the three sets. It seems that the biggest impact on the performance is given by a large training set (second partition) or alternatively by a large validation set. We could try to develop our model using other partitions and check if it is possible to get a better result. However, we prefered to move to the second part of the assignment and use a pretrained network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGLo3xPLoY7_"
      },
      "source": [
        "# 4. Using a pretrained network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY4e99Yig5ud"
      },
      "source": [
        "For the last section of the assignment we want to use a pretrained network, that is a network that was previously trained on a large dataset. If this original dataset is large enough and general enough, then the pretrained network can effectively act as a generic model and its features can prove useful for many different computer vision problems. Such portability of learned features across different problems is a key advantage of deep learning compared to other machine learning approaches.\n",
        "\n",
        "Let us consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and dogs. The architecture of the network is called VGG16. It’s a simple and widely used convnet architecture for ImageNet.\n",
        "\n",
        "There are two ways to use a pretrained network: \n",
        "- feature extraction\n",
        "- fine-tuning\n",
        "\n",
        "In this assignment, we are going to apply feature extraction, first without data augmentation and then including it to reach even better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy1iMqk3jucO"
      },
      "source": [
        "###Feature extraction - Instantiating the VGG16 convolutional base\n",
        "\n",
        "Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch. In other words, we are going to use the convolutional base, that is the series of pooling and convolution layers we had in our network, and then pass the output on a densely connected classifier that we choose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPGQSclNj08H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ca4ba9-8b79-4364-e3ce-2a329c9c732b"
      },
      "source": [
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(180, 180, 3))\n",
        "conv_base.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 180, 180, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 180, 180, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 180, 180, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 90, 90, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 90, 90, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 90, 90, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 45, 45, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 45, 45, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 45, 45, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 45, 45, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 22, 22, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 22, 22, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 22, 22, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 22, 22, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 11, 11, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCqmmX7Ok5Al"
      },
      "source": [
        "This convolutional base has 14.7 million parameters. Notice that we started with the same input we used previously, 3D tensors of shape (180, 180, 3), and we ended up with a tensor of shape (5, 5, 512)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pybcvUfCms_n"
      },
      "source": [
        "###Feature extraction - Extracting features and corresponding labels\n",
        "\n",
        "Here we preprocess the images before they are sent to the pretrained network. We can check the shape of the output and see that it corrisponds to what we saw in the network structure: (5, 5, 512)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LsTUSZbm0g9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1670138d-a7c9-4429-ffb2-044e331c9869"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_features_and_labels(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for images, labels in dataset:\n",
        "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "        features = conv_base.predict(preprocessed_images)\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_featuresPT1, train_labelsPT1 =  get_features_and_labels(train_dataset1)\n",
        "val_featuresPT1, val_labelsPT1 =  get_features_and_labels(validation_dataset1)\n",
        "test_featuresPT1, test_labelsPT1 =  get_features_and_labels(test_dataset1)\n",
        "\n",
        "train_featuresPT1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 5, 5, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lp_dxTNnDdh"
      },
      "source": [
        "###Feature extraction - Defining and training the densely connected classifier\n",
        "\n",
        "Now that we used the convolutional base of the pretrained network, we can feed the dense layer and train it. Notice that shape of the input in the following code is the same of the output of the convolutional base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEM892U5m3nQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ae06e1-f1f5-4745-a4ba-7c1bb14ee3da"
      },
      "source": [
        "inputsPT1 = keras.Input(shape=(5, 5, 512))\n",
        "x = layers.Flatten()(inputsPT1)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputsPT1 = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "modelPT1 = keras.Model(inputsPT1, outputsPT1)\n",
        "modelPT1.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacksPT1 = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"feature_extraction.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")\n",
        "]\n",
        "historyPT1 = modelPT1.fit(\n",
        "    train_featuresPT1, train_labelsPT1,\n",
        "    epochs=15,\n",
        "    validation_data=(val_featuresPT1, val_labelsPT1),\n",
        "    callbacks=callbacksPT1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "63/63 [==============================] - 3s 45ms/step - loss: 19.9267 - accuracy: 0.9195 - val_loss: 4.3865 - val_accuracy: 0.9700\n",
            "Epoch 2/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 3.8589 - accuracy: 0.9765 - val_loss: 3.6794 - val_accuracy: 0.9760\n",
            "Epoch 3/15\n",
            "63/63 [==============================] - 2s 39ms/step - loss: 1.8587 - accuracy: 0.9835 - val_loss: 5.0468 - val_accuracy: 0.9720\n",
            "Epoch 4/15\n",
            "63/63 [==============================] - 2s 40ms/step - loss: 1.0137 - accuracy: 0.9920 - val_loss: 7.7388 - val_accuracy: 0.9660\n",
            "Epoch 5/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 1.0241 - accuracy: 0.9895 - val_loss: 4.6645 - val_accuracy: 0.9730\n",
            "Epoch 6/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.5236 - accuracy: 0.9945 - val_loss: 3.8370 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.3154 - accuracy: 0.9950 - val_loss: 4.4557 - val_accuracy: 0.9740\n",
            "Epoch 8/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.5569 - accuracy: 0.9940 - val_loss: 5.8254 - val_accuracy: 0.9720\n",
            "Epoch 9/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.1800 - accuracy: 0.9985 - val_loss: 4.9876 - val_accuracy: 0.9740\n",
            "Epoch 10/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.6428 - accuracy: 0.9950 - val_loss: 7.0198 - val_accuracy: 0.9700\n",
            "Epoch 11/15\n",
            "63/63 [==============================] - 2s 37ms/step - loss: 0.2245 - accuracy: 0.9985 - val_loss: 6.0151 - val_accuracy: 0.9720\n",
            "Epoch 12/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.2414 - accuracy: 0.9970 - val_loss: 7.5349 - val_accuracy: 0.9650\n",
            "Epoch 13/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.2197 - accuracy: 0.9965 - val_loss: 4.8273 - val_accuracy: 0.9770\n",
            "Epoch 14/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 1.1091e-14 - accuracy: 1.0000 - val_loss: 4.8273 - val_accuracy: 0.9770\n",
            "Epoch 15/15\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.1465 - accuracy: 0.9990 - val_loss: 4.4155 - val_accuracy: 0.9760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD9g6wvNnMuQ"
      },
      "source": [
        "###Feature extraction - Plotting the results\n",
        "\n",
        "From the results we see that the accuracy improved notably from the performance we obtained when we trained a model from scratch using our limited dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7jJmO7lnSSu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "434cb684-091d-4209-ccf7-6b081124d070"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = historyPT1.history[\"accuracy\"]\n",
        "val_acc = historyPT1.history[\"val_accuracy\"]\n",
        "loss = historyPT1.history[\"loss\"]\n",
        "val_loss = historyPT1.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "byDGq50zCMlf",
        "outputId": "6bf5c0dd-18b0-4d3d-d992-2b5221e80585"
      },
      "source": [
        "test_modelPT1 = keras.models.load_model(\n",
        "    \"feature_extraction.keras\")\n",
        "test_loss, test_acc = test_modelPT1.evaluate(test_dataset1)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-84d0f87f3fd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_modelPT1 = keras.models.load_model(\n\u001b[1;32m      2\u001b[0m     \"feature_extraction.keras\")\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_modelPT1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test accuracy: {test_acc:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1330 test_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1267 test_step\n        y_pred = self(x, training=False)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py:269 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model_2: expected shape=(None, 5, 5, 512), found shape=(None, 180, 180, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8LEKEiCh7Nq"
      },
      "source": [
        "###Feature extraction with Data Augmentation\n",
        "\n",
        "Let us now add a data augmentation layer to the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWciqvLFCyE2"
      },
      "source": [
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)\n",
        "conv_base.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8AOg-HgC3n6",
        "outputId": "9dee75a1-3fae-42c4-f9e4-3e688149679d"
      },
      "source": [
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable weights before freezing the conv base: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6QxSz46C4NL",
        "outputId": "6faffcda-3147-4352-ed8b-ea8b7bb1efd6"
      },
      "source": [
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable weights after freezing the conv base: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61hfWQjbC5qv"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "inputsPT2 = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputsPT2)\n",
        "x = keras.applications.vgg16.preprocess_input(x)\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputsPT2 = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "modelPT2 = keras.Model(inputsPT2, outputsPT2)\n",
        "modelPT2.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F-Z1k_VDEn_",
        "outputId": "5384c983-9189-49c9-ea54-c8d3ae577bad"
      },
      "source": [
        "callbacksPT2 = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "historyPT2 = modelPT2.fit(\n",
        "    train_dataset1,\n",
        "    epochs=2,\n",
        "    validation_data=validation_dataset1,\n",
        "    callbacks=callbacksPT2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "63/63 [==============================] - 978s 16s/step - loss: 14.6821 - accuracy: 0.8980 - val_loss: 14.7979 - val_accuracy: 0.9250\n",
            "Epoch 2/2\n",
            "63/63 [==============================] - 976s 16s/step - loss: 7.2434 - accuracy: 0.9455 - val_loss: 3.6085 - val_accuracy: 0.9800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scdzNENaDEie",
        "outputId": "8d9e82e1-bef6-4635-dece-9cac866232ed"
      },
      "source": [
        "test_modelPT2 = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_modelPT2.evaluate(test_dataset1)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 324s 10s/step - loss: 4.2093 - accuracy: 0.9750\n",
            "Test accuracy: 0.975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "cd7NeXQviTt0",
        "outputId": "ec113cd6-178a-4516-8cce-5b2444b1acee"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_features_and_labels(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for images, labels in dataset:\n",
        "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "        features = conv_base.predict(preprocessed_images)\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_featuresPT2, train_labelsPT2 =  get_features_and_labels(train_dataset3)\n",
        "val_featuresPT2, val_labelsPT2 =  get_features_and_labels(validation_dataset3)\n",
        "test_featuresPT2, test_labelsPT2 =  get_features_and_labels(test_dataset3)\n",
        "\n",
        "train_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-d864b17e7af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_featuresPT2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labelsPT2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_features_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mval_featuresPT2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labelsPT2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_features_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_featuresPT2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labelsPT2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_features_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-d864b17e7af5>\u001b[0m in \u001b[0;36mget_features_and_labels\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpreprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fALEuYsjihYa"
      },
      "source": [
        "callbacksPT3 = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "historyPT3 = modelPT3.fit(\n",
        "    train_dataset3,\n",
        "    epochs=2,\n",
        "    validation_data=validation_dataset3,\n",
        "    callbacks=callbacksPT3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHeCYp62itAV"
      },
      "source": [
        "test_modelPT3 = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_modelPT3.evaluate(test_dataset3)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG6PU7IRue84"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2py_xCqups0"
      },
      "source": [
        ""
      ]
    }
  ]
}