{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3_Recurrent_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvoeIKTRb9Xq"
      },
      "source": [
        "# Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcMKxmRQb9Xr"
      },
      "source": [
        "## Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J765Bb43b9Xr"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI-eV_sZb9Xs"
      },
      "source": [
        "### Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pseqt05-b9Xs"
      },
      "source": [
        "### Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRvdXd5Rb9Xt"
      },
      "source": [
        "### Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_LMcQjCb9Xt"
      },
      "source": [
        "### Using the TextVectorization layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY0K4f8Mb9Xu"
      },
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48r3nhYTb9Xw",
        "outputId": "0a1968c7-c876-4fc9-d741-08216b3bc188"
      },
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc_uGrjOb9Xx",
        "outputId": "2f61b4fd-b232-4a71-cb6c-ac4f077afc18"
      },
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-TWgB11b9Xx"
      },
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHk9QjuLb9Xy"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4frYnTCb9Xy"
      },
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRy6-_vnb9Xz"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UH0GjoEb9Xz",
        "outputId": "87811e96-97d1-4cb1-9db5-b43c4e99c61a"
      },
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4pgQEKIb9Xz",
        "outputId": "dc769646-4173-4e44-97f4-0e50a6579120"
      },
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVnVktp4b9Xz",
        "outputId": "edc4e830-b226-47bd-9253-665ef1435d95"
      },
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3-1OTsMb9X0"
      },
      "source": [
        "## Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWWmLq5Tb9X0"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AhTN2t7b9X0",
        "outputId": "9799781f-4f3e-4a32-9e33-d384289b3de8"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  13.8M      0  0:00:05  0:00:05 --:--:-- 18.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSmBws2Cb9X0"
      },
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzIsJ909b9X1",
        "outputId": "149fd24c-27ed-49c1-bfbd-5a8e716bc32b"
      },
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwAwSEvab9X1"
      },
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uriIvQNSb9X1",
        "outputId": "2a3d0c67-9c55-492b-ef39-e6861f744c4b"
      },
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb7To077b9X1"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "147JwwcLb9X1",
        "outputId": "4ab637dd-b3d3-4362-8b4d-2ad2f2776d32"
      },
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"I love All Dogs Go to Heaven even though I'm a guy. This and The Land Before Time are the best animated films that Don Bluth has made! In the movie Gharlie Barkin (Burt Reynolds) is helped by his friend Itchy (Dom DeLuise) freedom out of the pound in New Orleans 1939. Charlie who's in casino business wants to share equal with his partner Carface (Vic Tayback). Carface who is unwilling to share the equal with Charlie pushes a car on a bridge onto Charlie killing him! Charlie enters heaven and meets Annabelle (Melba Moore) who shows Charlie that his time is up by showing him a watch that has stopped and she explains that All Dogs Go to Heaven because all dogs are naturally good! Charlie hides the stop watch behind his back and switches it back and he returns to Earth alive with Annabelle screaming You can never come back! Charlie reunites with Itchy and they go and explore soon to find out that Carface has not only attempted to murder Charlie he has also kidnapped a little orphan Girl named Ann Marie (Judith Barsi). When Carface leaves Charlie and Itchy help Ann Marie escape. The next day Charlie Itchy and Ann Marie go to look for money! Ann Marie sees a couple who she thinks would make great parents for her! While Ann Marie talks to the couple Charlie sneaks up behind the man and steals his wallet! Charlie Ann Marie and Itchy then go to a horse race where they bet the man's money that a horse will win the race! The horse they said would win winds up winning and Charlie, Itchy, and Ann Marie are payed $1,000 for the bet! Charlie promises Ann Marie he'll use the money to give to the poor but winds up buying a new casino and gambling and buys pizza for his friend Flo (Loni Anderson) and her puppies. Soon Ann Marie has found out that Charlie had stolen the wallet from the man and used his money on the horse race and everything! Charlie sad about this has a dream about going to Hell and the Devil! Soon Charlie awakens and finds out that Ann Marie is gone! She has left to give the wallet back to the couple who forgive her about the wallet and invite her to breakfast! Charlie asks Ann Marie to leave with him and she does pretending to be sick but are captured by mice who try to feed them to King Gator but they manage to escape! Soon Carface shows up and captures Ann Marie. He plans to drown her but Charlie comes to the rescue and calls on King Gator who eats Carface. Charlie's time is up and he must die again. Itchy with the help of the other dogs finds the couple who took Ann Marie in and get them to come with them to where Ann Marie is. They are there in time to save Ann Marie but are too late to save Charlie who's time has ended! Charlie who is awarded for his heroic effort for saving Ann Marie is welcomed back to Heaven but before he enters he says good-bye to Ann Marie who has been adopted by the couple and asks her to take care of Itchy for him. She says yes and tells Charlie she loves him and good-bye! Charlie enters Heaven again as it's said All Dogs Go to Heaven! Filled with wonderful animation, characters, and story Don Bluth has proved to us again that he is a good animator! It's too bad this movie was release the same year Little Mermaid which is my favorite Disney movie! They both came out in 1989 which was the year before I was born! I guess I'll have to call them both the Best Animated Features of 1989! 10 out of 10!\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAoEMvKGb9X2"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLyZT1i4b9X2"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJk_k9H0b9X2"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJFjedWWb9X2"
      },
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlOyMLpwb9X2"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lOUymy1b9X2",
        "outputId": "b8e914b3-be8c-464c-d45a-7deb1f6dc41f"
      },
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqkFUWSXb9X2"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEaRJazLb9X2"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weRoPTVsb9X3"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdhtwMnub9X3",
        "outputId": "238b22ba-91a6-4cd6-e658-a86fcbab2066"
      },
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 12s 18ms/step - loss: 0.3925 - accuracy: 0.8330 - val_loss: 0.2899 - val_accuracy: 0.8896\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2681 - accuracy: 0.9015 - val_loss: 0.3017 - val_accuracy: 0.8906\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2328 - accuracy: 0.9180 - val_loss: 0.3224 - val_accuracy: 0.8894\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2197 - accuracy: 0.9255 - val_loss: 0.3249 - val_accuracy: 0.8942\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2127 - accuracy: 0.9316 - val_loss: 0.3550 - val_accuracy: 0.8862\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2099 - accuracy: 0.9309 - val_loss: 0.3702 - val_accuracy: 0.8876\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2058 - accuracy: 0.9360 - val_loss: 0.3879 - val_accuracy: 0.8834\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2052 - accuracy: 0.9383 - val_loss: 0.3792 - val_accuracy: 0.8886\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2019 - accuracy: 0.9359 - val_loss: 0.3901 - val_accuracy: 0.8894\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2101 - accuracy: 0.9384 - val_loss: 0.4201 - val_accuracy: 0.8778\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2912 - accuracy: 0.8859\n",
            "Test acc: 0.886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAa0afjJb9X3"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4nO2CFkb9X3"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ZNSkzIb9X3"
      },
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73ub0mXsb9X3"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2GZpcg2b9X4",
        "outputId": "19a1d2c7-be32-4686-841f-c32380fc6059"
      },
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 20ms/step - loss: 0.3793 - accuracy: 0.8425 - val_loss: 0.2671 - val_accuracy: 0.9010\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2455 - accuracy: 0.9129 - val_loss: 0.2683 - val_accuracy: 0.9028\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2123 - accuracy: 0.9316 - val_loss: 0.2882 - val_accuracy: 0.9044\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2012 - accuracy: 0.9372 - val_loss: 0.3045 - val_accuracy: 0.9040\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1829 - accuracy: 0.9446 - val_loss: 0.3372 - val_accuracy: 0.9026\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1945 - accuracy: 0.9446 - val_loss: 0.3387 - val_accuracy: 0.9028\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1881 - accuracy: 0.9482 - val_loss: 0.3621 - val_accuracy: 0.8958\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.1842 - accuracy: 0.9488 - val_loss: 0.3705 - val_accuracy: 0.8958\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.1810 - accuracy: 0.9505 - val_loss: 0.3647 - val_accuracy: 0.8994\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1778 - accuracy: 0.9531 - val_loss: 0.3744 - val_accuracy: 0.8950\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.2671 - accuracy: 0.8999\n",
            "Test acc: 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN6-JISOb9X4"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDB038kob9X4"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlpQtz2Cb9X4"
      },
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEf9avyIb9X4"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4phpgOBhb9X4"
      },
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNy0VG3Tb9X4"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EvMvrkhb9X5",
        "outputId": "52afa3be-4ce9-4cf2-9392-bd49be3bb741"
      },
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 21ms/step - loss: 0.5008 - accuracy: 0.7804 - val_loss: 0.3039 - val_accuracy: 0.8892\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3403 - accuracy: 0.8670 - val_loss: 0.3428 - val_accuracy: 0.8538\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3017 - accuracy: 0.8810 - val_loss: 0.3305 - val_accuracy: 0.8718\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2811 - accuracy: 0.8867 - val_loss: 0.3679 - val_accuracy: 0.8430\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2713 - accuracy: 0.8903 - val_loss: 0.3564 - val_accuracy: 0.8514\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2600 - accuracy: 0.8956 - val_loss: 0.3566 - val_accuracy: 0.8692\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2512 - accuracy: 0.9007 - val_loss: 0.3508 - val_accuracy: 0.8710\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2419 - accuracy: 0.9046 - val_loss: 0.3590 - val_accuracy: 0.8590\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2339 - accuracy: 0.9086 - val_loss: 0.3920 - val_accuracy: 0.8574\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2294 - accuracy: 0.9109 - val_loss: 0.3669 - val_accuracy: 0.8548\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.3069 - accuracy: 0.8821\n",
            "Test acc: 0.882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGVdZ26Sb9X5"
      },
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk-N27xnb9X5",
        "outputId": "6167a26a-43a1-4443-90ac-e786b8a21391"
      },
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83.92 percent positive\n"
          ]
        }
      ]
    }
  ]
}